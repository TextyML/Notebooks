{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk # language processing\n",
    "import math\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "with open(\"/home/retkowski/data/bbc/business/Great Western rail modernisation costs rocket, says NAO.txt\") as f:\n",
    "    string = f.read()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens:  342\n",
      "Number of Occurences:  782\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Tokens: \", len(set(word_tokenize(string))))\n",
    "print(\"Number of Occurences: \", len(word_tokenize(string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ttr(text):\n",
    "    return len(set(word_tokenize(string))) / len(word_tokenize(string))\n",
    "\n",
    "def aq(text):\n",
    "    words = word_tokenize(string)\n",
    "    taggedWords = nltk.pos_tag(words)\n",
    "    adjectives = [a[0] for a in taggedWords if a[1] in ['JJ', 'JJR', 'JJS']]\n",
    "    verbs = [a[0] for a in taggedWords if a[1] in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']]\n",
    "    return len(verbs) / len(adjectives)\n",
    "\n",
    "def naq(text):\n",
    "    words = word_tokenize(string)\n",
    "    taggedWords = nltk.pos_tag(words)\n",
    "    adjectives = [a[0] for a in taggedWords if a[1] in ['JJ', 'JJR', 'JJS']]\n",
    "    verbs = [a[0] for a in taggedWords if a[1] in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']]\n",
    "    return len(verbs) / (len(adjectives) + len(verbs))\n",
    "\n",
    "def hl(text):\n",
    "    words = word_tokenize(string)\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    hapaxes = fdist.hapaxes()\n",
    "    return len(hapaxes) / len(words)\n",
    "\n",
    "def koi(text, n):\n",
    "    words = word_tokenize(text)\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    sum = 0\n",
    "    for word in fdist.most_common(n):\n",
    "        sum += word[1]\n",
    "    return sum / len(words)\n",
    "\n",
    "def nkoi(text, n, m):\n",
    "    words = word_tokenize(text)\n",
    "    h = math.floor(len(words) / m)\n",
    "    sum = 0\n",
    "    for i in range(h):\n",
    "        sum += nkoi_i(words[i*100:(i+1)*100],n)\n",
    "    return (sum/h)\n",
    "        \n",
    "def nkoi_i(words, n):\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    sum = 0\n",
    "    for word in fdist.most_common(n):\n",
    "        sum += word[1]\n",
    "    return sum / len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Kennzahlen aus der Linguistik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type-Token-Relation:  0.4373401534526854 \n",
      "\n",
      "Aktionsquotient:  2.180327868852459\n",
      "Aktionsquotient (normalisiert):  0.6855670103092784 \n",
      "\n",
      "Einmaligkeitsindex (Hapax legomenon):  0.289002557544757 \n",
      "\n",
      "Konzentrationsindex:  0.2659846547314578\n",
      "Konzentrationsindex (normalisiert):  0.33142857142857146\n"
     ]
    }
   ],
   "source": [
    "print(\"Type-Token-Relation: \", ttr(string), \"\\n\")\n",
    "print(\"Aktionsquotient: \", aq(string))\n",
    "print(\"Aktionsquotient (normalisiert): \", naq(string), \"\\n\")\n",
    "print(\"Einmaligkeitsindex (Hapax legomenon): \", hl(string), \"\\n\")\n",
    "print(\"Konzentrationsindex: \", koi(string, 10))\n",
    "print(\"Konzentrationsindex (normalisiert): \", nkoi(string, 10, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Morse', 'Richard', 'Morse', 'Meg']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import names\n",
    "words = word_tokenize(string)\n",
    "#namesInText = [name for name in names.words() if name in words]\n",
    "#namesInText\n",
    "namesInText = [word for word in words if word in names.words()]\n",
    "namesInText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity (NE) â€“ Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amyas Morse', 'Richard Westcott', 'Meg Hillier MP']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "    person_list = []\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        is_name = False\n",
    "        for part_of_name in person:\n",
    "             if part_of_name in namesInText:\n",
    "                    is_name = True\n",
    "        if is_name:\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "\n",
    "    return (person_list)\n",
    "\n",
    "get_human_names(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_entities(text,label):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "    entity_list = []\n",
    "    entity = []\n",
    "    name = \"\"\n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == label):\n",
    "        for leaf in subtree.leaves():\n",
    "            entity.append(leaf[0])\n",
    "        for part in entity:\n",
    "            name += part + ' '\n",
    "        if name[:-1] not in entity_list:\n",
    "            entity_list.append(name[:-1])\n",
    "        name = ''\n",
    "        entity = []\n",
    "\n",
    "    return (entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Great Western',\n",
       " 'National Audit',\n",
       " 'NAO',\n",
       " 'Department',\n",
       " 'Paddington',\n",
       " 'Maidenhead',\n",
       " 'FirstGroup',\n",
       " 'Transport']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_entities(string,\"ORGANIZATION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Image', 'England', 'London', 'South Wales', 'Scotland', 'Cardiff']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_entities(string,\"GPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
