{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BaseEstimator:** Muss get_feature_names, fit und transform implementieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LinguisticVectorizer(BaseEstimator):\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return np.array(['sent_neut', 'sent_pos', 'sent_neg',\n",
    "                         'nouns', 'adjectives', 'verbs', 'adverbs',\n",
    "                         'allcaps', 'exclamation', 'question'])\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def _get_sentiments(self, d):\n",
    "        # http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "        sent = tuple(nltk.word_tokenize(d))\n",
    "        if poscache is not None:\n",
    "            if d in poscache:\n",
    "                tagged = poscache[d]\n",
    "            else:\n",
    "                poscache[d] = tagged = nltk.pos_tag(sent)\n",
    "        else:\n",
    "            tagged = nltk.pos_tag(sent)\n",
    "\n",
    "        pos_vals = []\n",
    "        neg_vals = []\n",
    "\n",
    "        nouns = 0.\n",
    "        adjectives = 0.\n",
    "        verbs = 0.\n",
    "        adverbs = 0.\n",
    "\n",
    "        for w, t in tagged:\n",
    "            p, n = 0, 0\n",
    "            sent_pos_type = None\n",
    "            if t.startswith(\"NN\"):\n",
    "                sent_pos_type = \"n\"\n",
    "                nouns += 1\n",
    "            elif t.startswith(\"JJ\"):\n",
    "                sent_pos_type = \"a\"\n",
    "                adjectives += 1\n",
    "            elif t.startswith(\"VB\"):\n",
    "                sent_pos_type = \"v\"\n",
    "                verbs += 1\n",
    "            elif t.startswith(\"RB\"):\n",
    "                sent_pos_type = \"r\"\n",
    "                adverbs += 1\n",
    "\n",
    "            if sent_pos_type is not None:\n",
    "                sent_word = \"%s/%s\" % (sent_pos_type, w)\n",
    "\n",
    "                if sent_word in sent_word_net:\n",
    "                    p, n = sent_word_net[sent_word]\n",
    "\n",
    "            pos_vals.append(p)\n",
    "            neg_vals.append(n)\n",
    "\n",
    "        l = len(sent)\n",
    "        avg_pos_val = np.mean(pos_vals)\n",
    "        avg_neg_val = np.mean(neg_vals)\n",
    "\n",
    "        return [1 - avg_pos_val - avg_neg_val, avg_pos_val, avg_neg_val,\n",
    "                nouns / l, adjectives / l, verbs / l, adverbs / l]\n",
    "\n",
    "    def transform(self, documents):\n",
    "        obj_val, pos_val, neg_val, nouns, adjectives, verbs, adverbs = np.array(\n",
    "            [self._get_sentiments(d) for d in documents]).T\n",
    "\n",
    "        allcaps = []\n",
    "        exclamation = []\n",
    "        question = []\n",
    "\n",
    "        for d in documents:\n",
    "            allcaps.append(\n",
    "                np.sum([t.isupper() for t in d.split() if len(t) > 2]))\n",
    "\n",
    "            exclamation.append(d.count(\"!\"))\n",
    "            question.append(d.count(\"?\"))\n",
    "\n",
    "        result = np.array(\n",
    "            [obj_val, pos_val, neg_val, nouns, adjectives, verbs, adverbs, allcaps,\n",
    "             exclamation, question]).T\n",
    "\n",
    "        return result\n",
    "\n",
    "emo_repl = {\n",
    "    # positive emoticons\n",
    "    \"&lt;3\": \" good \",\n",
    "    \":d\": \" good \",  # :D in lower case\n",
    "    \":dd\": \" good \",  # :DD in lower case\n",
    "    \"8)\": \" good \",\n",
    "    \":-)\": \" good \",\n",
    "    \":)\": \" good \",\n",
    "    \";)\": \" good \",\n",
    "    \"(-:\": \" good \",\n",
    "    \"(:\": \" good \",\n",
    "\n",
    "    # negative emoticons:\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" bad \",\n",
    "    \":(\": \" bad \",\n",
    "    \":S\": \" bad \",\n",
    "    \":-S\": \" bad \",\n",
    "}\n",
    "\n",
    "emo_repl_order = [k for (k_len, k) in reversed(\n",
    "    sorted([(len(k), k) for k in list(emo_repl.keys())]))]\n",
    "\n",
    "re_repl = {\n",
    "    r\"\\br\\b\": \"are\",\n",
    "    r\"\\bu\\b\": \"you\",\n",
    "    r\"\\bhaha\\b\": \"ha\",\n",
    "    r\"\\bhahaha\\b\": \"ha\",\n",
    "    r\"\\bdon't\\b\": \"do not\",\n",
    "    r\"\\bdoesn't\\b\": \"does not\",\n",
    "    r\"\\bdidn't\\b\": \"did not\",\n",
    "    r\"\\bhasn't\\b\": \"has not\",\n",
    "    r\"\\bhaven't\\b\": \"have not\",\n",
    "    r\"\\bhadn't\\b\": \"had not\",\n",
    "    r\"\\bwon't\\b\": \"will not\",\n",
    "    r\"\\bwouldn't\\b\": \"would not\",\n",
    "    r\"\\bcan't\\b\": \"can not\",\n",
    "    r\"\\bcannot\\b\": \"can not\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Donald Trump' 'Hillary Clinton' 'Fabian Retkowski']\n",
      "{'Donald Trump': 0, 'Fabian Retkowski': 0, 'Hillary Clinton': 0}\n",
      "{'Donald Trump': 0, 'Fabian Retkowski': 0, 'Hillary Clinton': 0}\n",
      "{'Donald Trump': 0, 'Fabian Retkowski': 0, 'Hillary Clinton': 0}\n",
      "[[1 0 0]\n",
      " [1 1 0]\n",
      " [0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "import nltk # language processing\n",
    "import math\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import names\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import operator\n",
    "\n",
    "class NamedEntityVectorizer(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, entity=\"PERSON\", max_features=None): #max_df=1.0, min_df=1\n",
    "        #self.max_df = max_df\n",
    "        #self.min_df = min_df\n",
    "        self.entity = entity\n",
    "        self.max_features = max_features\n",
    "        self.features = []\n",
    "        \n",
    "    def get_feature_names(self):\n",
    "        return np.array(self.features)\n",
    "\n",
    "    def _stanford_tagger(self, text):\n",
    "        ttext = word_tokenize(text)\n",
    "        st_tags = StanfordNERTagger('/opt/nltk_data/stanford/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "                                   '/opt/nltk_data/stanford/stanford-ner.jar', encoding='utf-8')\n",
    "        tagged = st_tags.tag(ttext)  \n",
    "\n",
    "        tagList = []\n",
    "        tempTag = None\n",
    "        for tag_index, tag in enumerate(tagged):   \n",
    "            if tag[1] != \"O\":\n",
    "                if tempTag is None:\n",
    "                    tempTag = tag\n",
    "                elif tempTag[1] == tag[1]:\n",
    "                    tempTag = (tempTag[0] + \" \"+ tag[0], tempTag[1])\n",
    "                else:\n",
    "                    tagList.append(tempTag)\n",
    "                    tempTag = tag\n",
    "            elif tempTag is not None:\n",
    "                tagList.append(tempTag)\n",
    "                tempTag = None\n",
    "\n",
    "        return(tagList)\n",
    "\n",
    "    def _count_vocab(self, raw_documents, build_doc):\n",
    "        vocabulary = {}\n",
    "        doc_counter = []\n",
    "        counter = {}\n",
    "        for doc in raw_documents:\n",
    "            tagged = self._stanford_tagger(doc)\n",
    "            list_of = [chunk for chunk in tagged if chunk[1] == self.entity]\n",
    "            list_set_of = list(set(list_of))\n",
    "            if build_doc:\n",
    "                doc_dict = {el:0 for el in self.features}\n",
    "                        \n",
    "            for feature in list_of:\n",
    "                try:\n",
    "                    if build_doc:\n",
    "                        doc_dict[feature[0]] += 1\n",
    "                    vocabulary[feature[0]] += 1\n",
    "                except KeyError:\n",
    "                    vocabulary[feature[0]] = 1\n",
    "                    \n",
    "            for feature in list_set_of:\n",
    "                try:\n",
    "                    counter[feature[0]] += 1\n",
    "                except KeyError:\n",
    "                    counter[feature[0]] = 1\n",
    "                    \n",
    "            if build_doc:        \n",
    "                doc_counter.append(doc_dict)\n",
    "                        \n",
    "        return vocabulary, counter, doc_counter\n",
    "        \n",
    "    def fit(self, raw_documents, y=None):\n",
    "        v, c, d = self._count_vocab(raw_documents, False)\n",
    "        if self.max_features is None or len(v) > self.max_features:\n",
    "            max = len(v)\n",
    "        sorted_v = sorted(v.items(), key=lambda x: x[1], reverse=True)[:self.max_features]\n",
    "        self.features = [p[0] for p in sorted_v]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, raw_documents):\n",
    "        v, c, d = self._count_vocab(raw_documents, True)\n",
    "        result = np.array([list(doc.values()) for doc in d])\n",
    "        return result\n",
    "\n",
    "    \n",
    "nerv = NamedEntityVectorizer(max_features=5)\n",
    "\n",
    "texts = [\"Donald Trump is made by Hillary Trump\", \"Fabian Retkowski is a person made by Donald Trump.\", \"Hillary Clinton is not a person, but Hillary Clinton is.\"]\n",
    "            \n",
    "nerv.fit(texts)\n",
    "print(nerv.get_feature_names())\n",
    "print(nerv.transform(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
