{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk # language processing\n",
    "import os # operation system\n",
    "import numpy as np\n",
    "import re, collections\n",
    "import nltk.corpus\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unusual_words(text):\n",
    "    text_vocub = set(w.lower() for w in word_tokenize(text) if w.isalpha())\n",
    "    text_dict  = set(w.lower() for w in WORDS)\n",
    "    unusual = text_vocub - text_dict\n",
    "    return sorted(unusual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_unusual_words(text):\n",
    "    text_vocub = set(w.lower() for w in word_tokenize(text) if w.isalpha())\n",
    "    text_dict  = set(w.lower() for w in WORDS)\n",
    "    unusual = text_vocub - text_dict\n",
    "    return len(unusual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/home/retkowski/data/bbc/business/Great Western rail modernisation costs rocket, says NAO.txt\") as f:\n",
    "    text = f.read()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unusual words after converting to lowercase\n",
      "['firstgroup', 'nao', 'westcott']\n",
      "---\n",
      "Unusual words without conversion\n",
      "['Among', 'Amyas', 'Analysis', 'And', 'Audit', 'Because', 'Before', 'But', 'Cardiff', 'Correspondent', 'Department', 'England', 'FirstGroup', 'Great', 'Hillier', 'However', 'I', 'Image', 'It', 'London', 'MP', 'Maidenhead', 'Meanwhile', 'Meg', 'Modernisation', 'Morse', 'Mr', 'NAO', 'National', 'Network', 'Office', 'PA', 'Paddington', 'Rail', 'Rails', 'Richard', 'Scotland', 'So', 'South', 'The', 'They', 'Transport', 'Wales', 'Westcott', 'Western', 'When', 'Yesterday']\n"
     ]
    }
   ],
   "source": [
    "text_vocub = set(w.lower() for w in word_tokenize(text) if w.isalpha())\n",
    "text_dict  = set(w.lower() for w in WORDS)\n",
    "print(\"Unusual words after converting to lowercase\")\n",
    "print(sorted(text_vocub - text_dict))\n",
    "print(\"---\")\n",
    "print(\"Unusual words without conversion\")\n",
    "text_vocub = set(w for w in word_tokenize(text) if w.isalpha())\n",
    "text_dict  = set(w for w in WORDS)\n",
    "print(sorted(text_vocub - text_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['i', 'like', 'trains', 'i', 'll', 'win']\n",
      "['I', 'like', 'trains', '.', 'I', \"'ll\", 'win', '!']\n",
      "['wiin']\n"
     ]
    }
   ],
   "source": [
    "print(count_unusual_words(text))\n",
    "\n",
    "print(words(\"I like trains. I'll win!\"))\n",
    "print(word_tokenize(\"I like trains. I'll win!\"))\n",
    "\n",
    "print(unusual_words(\"I'll wiin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('/home/retkowski/text_analyse/dicts/dict_all.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word', 'wort'}\n"
     ]
    }
   ],
   "source": [
    "print(known([\"wort\",\"word\",\"accont\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soothing\n"
     ]
    }
   ],
   "source": [
    "print(correction(\"somthing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'soothing', 'southing', 'something', 'sowthing'}\n"
     ]
    }
   ],
   "source": [
    "print(known(edits1(\"somthing\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502392\n"
     ]
    }
   ],
   "source": [
    "print(len(WORDS))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "bc0d5528-5c29-497a-ad57-5b88f99a8c53",
    "theme": {}
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
